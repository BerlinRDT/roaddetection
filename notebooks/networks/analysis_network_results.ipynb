{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of network model results\n",
    "To do:\n",
    "* implement, test/check multi-label computations\n",
    "* implement breakeven point on PR summary curve -> use threshold at that point to generate labels\n",
    "* write labels to geotiffs to dir data/test/predict_process or so \n",
    "* implement masks for selecting no_img pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "from src.models.data import *\n",
    "from src.models.model import *\n",
    "from src.data.utils import get_tile_prefix\n",
    "from src.models.metrics_img import auc_roc\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import skimage.io as io\n",
    "from skimage import exposure\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from pathlib import Path\n",
    "import os, shutil\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to append\n",
    "sys.path.append(\"/home/ubuntu/roaddetection/\")\n",
    "sys.path.append(\"/media/hh/hd_internal/hh/DSR_Berlin_2018/roaddetection/\")\n",
    "\n",
    "# base directory with data (image tiles) to be analyzed\n",
    "# eval_dir = \"../../data/validate\"\n",
    "eval_dir = \"../../data/train\"\n",
    "# subdirs\n",
    "dir_x = 'sat'\n",
    "dir_y = 'map'\n",
    "\n",
    "# max. number of samples (files) to analyze (predicition takes a long time)\n",
    "max_num_x = 20\n",
    "\n",
    "# number of samples to plot in detail\n",
    "num_x_show = 10\n",
    "\n",
    "# size of images\n",
    "target_size = (512,512)\n",
    "#target_size = (256,256)\n",
    "\n",
    "# path to & filename of model to analyze\n",
    "trained_model_fn = '../../models/models_unet_only_borneo_generalized_03_09_12_33.hdf5'\n",
    "#trained_model_fn = '../../models/unet_borneo_manual_03_09.hdf5'\n",
    "\n",
    "# list any custom loss or metric functions of the model here\n",
    "custom_objects = {'auc_roc': auc_roc}\n",
    "\n",
    "# individual samples to be shown: either None, a list of indexes, or any of 'random', 'head_tail'\n",
    "# in the latter two cases, up to ten images will be picked; head_tail = the five best and the five worst-predicted\n",
    "show_samples = \"random\"\n",
    "#show_samples = \"head_tail\"\n",
    "\n",
    "# colormap to be used for prediction scores \n",
    "cmap_yscore = 'gnuplot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some constants, preparatory computations & definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label values\n",
    "CLASS_DICT = {\n",
    "    \"no_img\": 0,\n",
    "    \"no_road\": 40,\n",
    "    \"paved_road\": 127,\n",
    "    \"unpaved_road\": 255\n",
    "}\n",
    "\n",
    "# colors and other graphics properties to be used for plotting\n",
    "CLASS_PLOT_PROP = {\n",
    "    \"no_road\": [\"gray\"],\n",
    "    \"paved_road\": [\"navy\"],\n",
    "    \"unpaved_road\": [\"darkorange\"],\n",
    "    \"avg\": [\"black\"]\n",
    "}\n",
    "\n",
    "# list of satellite image files & their number\n",
    "_, _, file_list_x = next(os.walk(os.path.join(eval_dir, dir_x)))\n",
    "num_x = len(file_list_x)\n",
    "\n",
    "# actual number of files that will be analyzed, given files available\n",
    "num_x_use = min(num_x, max_num_x)\n",
    "\n",
    "# actual number of samples that can be shown, given number of samples to be analyzed\n",
    "num_x_show = np.min([num_x_show, num_x_use])\n",
    "\n",
    "def show_tile(tile, ax, cmap=None, show_colorbar=False, title=None, **kwargs):\n",
    "    im_h = ax.imshow(tile, cmap=cmap, **kwargs);\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    if show_colorbar:\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        plt.colorbar(im_h, cax=cax)\n",
    "    ax.set(title=title)\n",
    "    return im_h\n",
    "\n",
    "def multiclass_roc_pr(y, yscore, class_dict=CLASS_DICT, curve=True):\n",
    "    \"\"\"\n",
    "    Perform binary or multi-class roc and pr analyses in which all classes are compared\n",
    "    to the \"no_road\" class and data of the \"no_img\" class are ignored\n",
    "    y - 1D array of labels\n",
    "    yscore - array of prediction scores\n",
    "    class_dict - dictionary listing all legal values in y\n",
    "    curve - flag indicating whether fpr and tpr for display of curve are to be computed\n",
    "    \"\"\"\n",
    "    debug_mode = False\n",
    "    # - allocate\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    pr_auc = dict() \n",
    "    reduced_class_dict = dict()\n",
    "    # sanity check: make sure all labels in y actually exist in class_dict:\n",
    "    unique_labels = np.unique(y)    \n",
    "    assert(not set(unique_labels).difference(set(class_dict.values()))), \"illegal label\"\n",
    "    #--------------------------------------------------------------------------\n",
    "    # only if any class other than no_img and no_road are present do we proceed\n",
    "    #--------------------------------------------------------------------------\n",
    "    if len(set(unique_labels).difference((class_dict[\"no_img\"], class_dict[\"no_road\"]))):\n",
    "        # remove all entries corresponding to no_img because they are pointless\n",
    "        good_ix = y != class_dict[\"no_img\"]\n",
    "        if debug_mode:\n",
    "            print(\"excluding {0:0.0f} % non-image pixels)...\".format(100*(1.0 - np.sum(good_ix)/y.size)))\n",
    "            print(good_ix)\n",
    "        y = y[good_ix]\n",
    "        yscore = yscore[good_ix]\n",
    "        if debug_mode:\n",
    "            print(y)    \n",
    "        # consider only labels which exist in y\n",
    "        unique_labels = np.unique(y)\n",
    "        reduced_class_dict = {list(class_dict.keys())[i]:list(class_dict.values())[i] \\\n",
    "                      for i, val in enumerate(list(class_dict.values())) if val in unique_labels}\n",
    "        num_label = len(reduced_class_dict)\n",
    "        if debug_mode:\n",
    "            print(unique_labels, reduced_class_dict)\n",
    "\n",
    "\n",
    "        # to do: remove corresponding layers (?) in yscore    \n",
    "        if yscore.ndim >1:\n",
    "            raise Exception(\"multi-label scores not yet implemented\")\n",
    "\n",
    "        # binarize on all categories found in y\n",
    "        y_multilabel = label_binarize(y, list(reduced_class_dict.values()))\n",
    "        # Compute ROC curve and ROC area for each non-no_road class:\n",
    "\n",
    "        # keys for fpr, tpr and roc_auc, indicating the class which is tested against no_road\n",
    "        keys = [k for k in reduced_class_dict.keys() if k != \"no_road\"]\n",
    "        # if it's a binary problem, y_multilabel is [nsamples x 1]\n",
    "        if num_label == 2:\n",
    "            fpr[keys[0]], tpr[keys[0]], _ = metrics.roc_curve(y_multilabel, yscore)\n",
    "            roc_auc[keys[0]] = metrics.auc(fpr[keys[0]], tpr[keys[0]])\n",
    "            precision[keys[0]], recall[keys[0]], _ = \\\n",
    "                    metrics.precision_recall_curve(y_multilabel, yscore)\n",
    "            pr_auc[keys[0]] = metrics.auc(precision[keys[0]], recall[keys[0]], reorder=True)\n",
    "            \n",
    "            \n",
    "        elif num_label >= 3:\n",
    "            raise Exception(\"multilabel computation of auroc not yet implemented\")\n",
    "            for i in range(1, num_label):\n",
    "                pass\n",
    "            # assign averages  \n",
    "            #fpr[\"avg\"], tpr[\"avg\"], roc_auc[\"avg\"] = fpr[keys[0]], tpr[keys[0]], roc_auc[keys[0]]\n",
    "    else:\n",
    "        print(\"skipping computations due to absence of labels of interest\" )\n",
    "    return fpr, tpr, roc_auc, precision, recall, pr_auc, reduced_class_dict\n",
    "\n",
    "def plot_roc(fpr_dict, tpr_dict, auc_roc_dict, ax, plot_prop=CLASS_PLOT_PROP):\n",
    "    if len(fpr_dict):\n",
    "        # plot diagonal\n",
    "        ax.plot(np.linspace(0, 1), np.linspace(0, 1), linestyle='--', color='gray')\n",
    "        for k in fpr_dict.keys():\n",
    "            ax.plot(fpr_dict[k], tpr_dict[k],\n",
    "                    label=\"{0:s}: auc = {1:0.2f}\".format(str(k), auc_roc_dict[k]),\n",
    "                    color = plot_prop[k][0])\n",
    "        ax.set_yticks(np.arange(0, 1.25, 0.25))\n",
    "        ax.set_xticks(np.arange(0, 1.25, 0.25))\n",
    "        ax.set_ylim(-0.01, 1.01)\n",
    "        ax.set_xlim(-0.01, 1.01)\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.set(title='ROC', xlabel='false positive rate', ylabel='true positive rate')\n",
    "\n",
    "        \n",
    "def plot_pr(recall_dict, precision_dict, auc_pr_dict, ax, plot_prop=CLASS_PLOT_PROP):\n",
    "    if len(recall_dict):\n",
    "        for k in recall_dict.keys():\n",
    "            ax.plot(recall_dict[k], precision_dict[k], \\\n",
    "                    label=\"{0:s}: auc = {1:0.2f}\".format(str(k), auc_pr_dict[k]),\n",
    "                    color = plot_prop[k][0])\n",
    "        ax.set_yticks(np.arange(0, 1.25, 0.25))\n",
    "        ax.set_xticks(np.arange(0, 1.25, 0.25))\n",
    "        ax.set_ylim(-0.01, 1.01)\n",
    "        ax.set_xlim(-0.01, 1.01)\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.set(title='PR', xlabel='recall', ylabel='precision')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load complete model\n",
    "The additional input arg \"custom_objects\" is needed if custom loss or metrics were used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(trained_model_fn, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over files, collecting data & predicitions (takes a long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of pixels per image\n",
    "img_size = np.prod(target_size)\n",
    "# preallocate arrays collecting the label (y) values and y scores of all samples\n",
    "arr_y = np.empty(img_size * num_x_use, dtype=np.uint8)\n",
    "arr_yscore = np.empty(img_size * num_x_use, dtype=np.float32)\n",
    "# array collecting the key metric for each sample individually\n",
    "arr_metric = np.empty(num_x_use)\n",
    "\n",
    "for i, fn in enumerate(file_list_x[:num_x_use]):\n",
    "    # read sat image tile\n",
    "    x = io.imread(os.path.join(eval_dir, dir_x, fn))\n",
    "    # -------------------------------------------------------------------------------- \n",
    "    # revise this part if/once non-covered parts of image tiles are labeled as such\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # determine invalid pixels (for now defined as those with a vale of zero\n",
    "    # in the first band). Variable mask could be used to create a masked array,\n",
    "    # but scikit-learn does not support masked arrays\n",
    "    mask = x[:,:,0] == 0;\n",
    "    # scale x\n",
    "    x = x/255.0\n",
    "    # read corresponding label tile\n",
    "    y = io.imread(os.path.join(eval_dir, dir_y, fn))\n",
    "    # set masked values: first, set zeros in label file to 'no road' value...\n",
    "    y[np.logical_and(np.logical_not(mask), np.logical_not(y))] = CLASS_DICT[\"no_road\"]\n",
    "    # then set pixel positions found to not belong to image to 'no_img' value\n",
    "    y[mask] = CLASS_DICT[\"no_img\"]\n",
    "    # copy flattened labels in array\n",
    "    arr_y[i*img_size:(i+1)*img_size] = y.ravel()\n",
    "    # predict\n",
    "    print(\"analyzing {0:s} ({1:0.0f} % non-image pixels)...\".format(fn, 100*np.sum(mask)/img_size))\n",
    "    yscore = model.predict(x.reshape((1,) + target_size +(4,)))\n",
    "    # copy flattened prediction in array\n",
    "    arr_yscore[i*img_size:(i+1)*img_size] = yscore.ravel()\n",
    "    # compute and store metric used for sorting\n",
    "    _, _, roc_auc_dict, _, _, pr_auc_dict, _ = multiclass_roc_pr(y.ravel(), yscore.ravel())\n",
    "    if len(pr_auc_dict) == 0:\n",
    "        arr_metric[i] = None\n",
    "    elif len(pr_auc_dict) == 1:\n",
    "        # binary labels\n",
    "        arr_metric[i] = pr_auc_dict[list(pr_auc_dict.keys())[0]]\n",
    "    else:\n",
    "        # pick average\n",
    "        arr_metric[i] = pr_auc_dict[\"avg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare index for showing samples\n",
    "if show_samples == \"random\":\n",
    "    samples_ix = np.random.choice(num_x_use, num_x_show, replace=False)\n",
    "elif show_samples == \"head_tail\":\n",
    "    # indexes to best and worst examples\n",
    "    ix_sorted = np.argsort(arr_metric)\n",
    "    samples_ix = np.hstack((ix_sorted[:(num_x_show//2)],ix_sorted[(-num_x_show//2):]))\n",
    "elif type(show_samples) is list:\n",
    "    samples_ix = np.array(show_samples, dtype=int)\n",
    "    samples_ix = samples_ix[samples_ix < num_x_use]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and plot metrics on ensemble of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_dict, tpr_dict, roc_auc_dict, precision_dict, recall_dict, pr_auc_dict, reduced_class_dict = \\\n",
    "        multiclass_roc_pr(arr_y, arr_yscore)\n",
    "\n",
    "# set up summary figure\n",
    "fig_sum, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "plot_pr(recall_dict, precision_dict, pr_auc_dict, axs[0, 0])\n",
    "plot_roc(fpr_dict, tpr_dict, roc_auc_dict, axs[0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show individual samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ix in samples_ix:\n",
    "    fn = file_list_x[ix]\n",
    "    # read sat image tile\n",
    "    x = io.imread(os.path.join(eval_dir, dir_x, fn))\n",
    "    # retrieve labels\n",
    "    y = arr_y[ix*img_size:(ix+1)*img_size]\n",
    "    # retrieve y score (prediction)\n",
    "    yscore = arr_yscore[ix*img_size:(ix+1)*img_size]\n",
    "    # make a copy for display in which we get rid of upper small percentile \n",
    "    yscore_plot = np.copy(yscore).reshape(target_size)\n",
    "    prc = np.percentile(yscore_plot, [99.9])\n",
    "    yscore_plot[yscore_plot >= prc] = prc\n",
    "    # ----------------set up figure ---------------\n",
    "    fig_sample, axs = plt.subplots(2, 4, figsize=(15, 7))\n",
    "    fig_sample.suptitle(fn, fontsize=16)\n",
    "    # plot rgb part of image\n",
    "    show_tile(x[:,:,[2, 1, 0]], axs[0,0], title=\"RGB\");\n",
    "    # nir\n",
    "    show_tile(x[:,:,3], axs[0,1], cmap=\"gray\",  title=\"infrared\");\n",
    "    # labels § to be replaced by Lisa's code\n",
    "    show_tile(y.reshape(target_size), axs[0,2], cmap=\"gray\",  title=\"labels\");\n",
    "    # y score (prediction)\n",
    "    show_tile(yscore_plot, axs[0,3], cmap=cmap_yscore, show_colorbar=True,  title=\"prediction\");\n",
    "    # pale rgb + prediction\n",
    "    show_tile(exposure.adjust_gamma(x[:,:,[2, 1, 0]], 0.5), axs[1,0]);\n",
    "    show_tile(yscore_plot, axs[1,0], cmap=cmap_yscore, title=\"rgb + prediction\", alpha=.5);\n",
    "    # auc_roc, auc_pr\n",
    "    fpr_sample_dict, tpr_sample_dict, roc_auc_sample_dict, \\\n",
    "        precision_sample_dict, recall_sample_dict, pr_auc_sample_dict, \\\n",
    "        reduced_label_sample_dict = multiclass_roc_pr(y, yscore)\n",
    "    plot_pr(recall_sample_dict, precision_sample_dict, pr_auc_sample_dict, axs[1, 1])\n",
    "    plot_roc(fpr_sample_dict, tpr_sample_dict, roc_auc_sample_dict, axs[1, 2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# halt\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outdated stuff\n",
    "which is not used currently but may come in handy later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick test of multiclass_roc\n",
    "multiclass_roc(np.r_[0, 40, 40, 0, 255, 255, 0, 255], np.empty(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input arguments to Keras' ImageDataGenerator - be sure not to include any image augmentation here!\n",
    "data_gen_args = dict(data_format=\"channels_last\")\n",
    "\n",
    "# batch size for summary stats without visualization (the more, the more efficient, but limited by memory)\n",
    "batch_size = 3\n",
    "\n",
    "\n",
    "# 'steps' input par into evaluate_generator\n",
    "steps =  num_x_use // batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation: only numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up test gen with a batch size as large as possible for efficiency reasons\n",
    "test_gen = trainGenerator(batch_size, eval_dir, img_dir, label_dir,\n",
    "                        data_gen_args, save_to_dir = None, image_color_mode=\"rgba\", target_size=target_size)\n",
    "res = model.evaluate_generator(test_gen, steps=steps, workers=1, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prediction for display of images and more sophisticated evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_generator(test_gen, steps=steps, workers=1, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred[5].reshape(target_size), cmap='gray');\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generator is supposed to yield single images and matching labels, hence batch size = 1\n",
    "#batch1_test_gen = trainGenerator(1, eval_dir, img_dir, label_dir,\n",
    "#                        data_gen_args, save_to_dir = None, image_color_mode=\"rgba\", target_size=target_size)\n",
    "# preallocate linear arrays for collecting flattened predicition and label data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
