{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from keras.layers import merge\n",
    "from src.models.catdata import *\n",
    "from src.models.catmodel import *\n",
    "from src.data.utils import get_tile_prefix\n",
    "from src.models.metrics_img import auc_roc\n",
    "#import rasterio.plot as rioplot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "\n",
    "from pathlib import Path\n",
    "import os, shutil\n",
    "import sys\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import backend as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and make (if necessary) train/validation/test directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../visualize_imagery/numOfPixPerClassPerTile_256.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937\n"
     ]
    }
   ],
   "source": [
    "sdf = df[((df['relative_noRoad']) < 0.95) \n",
    "         & ((df['relative_pavedRoad']) > 0.0)\n",
    "        ].\\\n",
    "        reset_index(drop=True)\n",
    "sdf = shuffle(sdf)\n",
    "print(len(sdf))\n",
    "sdf, _  = train_test_split(sdf, test_size=0.5)\n",
    "train_tmp, test = train_test_split(sdf, test_size=0.2)\n",
    "train, valid = train_test_split(train_tmp, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Harz      92\n",
       "Borneo     2\n",
       "Name: region, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.region.value_counts()\n",
    "#test = test[(test['region'] == 'Borneo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2170.0, 2750.5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.numPixel_unpavedRoad.median(), sdf.numPixel_pavedRoad.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.034462268535907455, 0.05735038692115718)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.relative_unpavedRoad.mean(), sdf.relative_pavedRoad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1758985, 1056987)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.numPixel_pavedRoad.sum(), sdf.numPixel_unpavedRoad.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_weight(labels_dict,mu=0.25):\n",
    "    total = sum(labels_dict.values())\n",
    "    print(total)\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "    weights = []\n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "        weights.append(score if score > 1.0 else 1.0)\n",
    "\n",
    "    return weights#class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65536.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 1.5035261910022688, 1.9665871942287558]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeldict = {\n",
    "    0: train.numPixel_noRoad.mean(),\n",
    "    1: train.numPixel_pavedRoad.mean(),\n",
    "    2: train.numPixel_unpavedRoad.mean()\n",
    "}\n",
    "\n",
    "create_class_weight(labeldict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_loss(y_true, y_pred):\n",
    "    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
    "    return -K.sum(y_true * K.log(y_pred), axis=-1)\n",
    "\n",
    "def categoricalCrossentropy(y_true, y_pred, the0, the1):\n",
    "    '''\n",
    "    Calculate the class-weighted categorical cross-entropy for the given\n",
    "    predicted and true sets.\n",
    "\n",
    "    y_true [in] The truth set to test against. This is a Tensor with a last\n",
    "                dimension that contains a set of 1-of-N selections.\n",
    "    y_pred [in] The predicted set to test against. This is a Tensor with a last\n",
    "                dimension that contains a set of 1-of-N selections.\n",
    "    returns     A Tensor function that will calculate the weighted categorical\n",
    "                cross-entropy on the inputs.\n",
    "    '''\n",
    "\n",
    "    P = np.array([[1, the1, the1], \n",
    "                  [the0, 1, the1], \n",
    "                  [the1, the0, 1]])\n",
    "    P = P/np.linalg.norm(P)\n",
    "\n",
    "    P_inv = K.constant(np.linalg.inv(P))\n",
    "    # If weights are defined, multiply the truth values by the class weights.\n",
    "    #\n",
    "    if P_inv is not None:\n",
    "        # Wrap the loss weights in a tensor object.\n",
    "        #\n",
    "        pp = P# np.linalg.inv(P)\n",
    "        theWeights =  K.constant(pp, shape=pp.shape)\n",
    "\n",
    "        y_true = K.dot(y_true,theWeights)\n",
    "\n",
    "    # Get the cross-entropy and return it.\n",
    "    #\n",
    "    crossEntropy =K.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    return crossEntropy\n",
    "\n",
    "def dice_loss(the0, the1):    \n",
    "    def loss(y_true, y_pred):\n",
    "        return categoricalCrossentropy(y_true, y_pred, the0, the1)\n",
    "    return loss\n",
    "    \n",
    "def pixel_wise_loss(y_true, y_pred):\n",
    "    pos_weight = tf.constant(P_inv)\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        pos_weight,\n",
    "        name=None\n",
    "    )\n",
    "\n",
    "    return K.mean(loss,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir = \"../../data/train_raw\"\n",
    "raw_images_path = \"../../data/raw/images\"\n",
    "dirs = []\n",
    "\n",
    "base_dir = \"../../data\"\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "dirs.append(train_dir)\n",
    "validation_dir = os.path.join(base_dir, \"validate\")\n",
    "dirs.append(validation_dir)\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "dirs.append(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in dirs:\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to append\n",
    "sys.path.append(\"/home/ubuntu/roaddetection/\")\n",
    "\n",
    "# ------------- image characteristics and augmentation -----------------------------\n",
    "# size of tiles\n",
    "target_size = (256, 256)\n",
    "# input arguments to Keras' ImageDataGenerator\n",
    "data_gen_args = dict(\n",
    "    data_format=\"channels_last\",\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    " )\n",
    "# if True, image tiles will be split up into training/validation/test sets\n",
    "# only required after a fresh 'make data'\n",
    "do_data_split = True\n",
    "# directory into which to place *training* images from ImageDataGenerator for inspection;\n",
    "# default should be None because this slows things down\n",
    "imgdatagen_dir = None\n",
    "#imgdatagen_dir = '../../data/imgdatagenerator'\n",
    "\n",
    "#--------------- network weights ----------------------------------------------------\n",
    "# path to & filename of pre-trained model to use - set to None if you want to start from scratch\n",
    "# pretrained_model_fn = \"../../models/unet_membrane_analytic_27_08_14_55.hdf5\"\n",
    "pretrained_model_fn = False\n",
    "#pretrained_model_fn = '../../models/newBorneo_multiCat_unet.hdf5'\n",
    "\n",
    "# path to & filename of model to save\n",
    "trained_model_fn = '../../models/0904_0148_NL_Borneo_multiCat_unet.hdf5'\n",
    "\n",
    "#--------------- training details / hyperparameters -----------------------------------\n",
    "# batch size\n",
    "batch_size = 3\n",
    "# steps per epoch, should correspond to [number of training images] / batch size\n",
    "steps_per_epoch = len(train)# * batch_size\n",
    "# number of epochs\n",
    "epochs = 5\n",
    "# number of steps on validation set\n",
    "validation_steps = len(valid) * batch_size\n",
    "# self-explanatory variables:\n",
    "optimizer    = Adam(lr=1e-4)\n",
    "#loss         = categoricalCrossentropy#'categorical_crossentropy'\n",
    "loss_weights = None\n",
    "metrics      = ['accuracy', auc_roc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data up into train/validation/test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20170815_005028_0c0b_3B', '20180419_074326_0c43_3B', '20180427_020346_103a_3B', '20180504_094435_0e19_3B', '20180419_074323_0c43_3B', '20180427_020502_103c_3B', '20180427_020347_103a_3B', '20171013_232848_0c46_3B', '20180427_020501_103c_3B', '20180310_020203_1040_3B', '20180419_074324_0c43_3B', '20170815_005030_0c0b_3B', '20180419_074325_0c43_3B', '20180310_020202_1040_3B', '20180427_020504_103c_3B', '20180606_020625_0f1b_3B', '20180419_074324_1_0c43_3B', '20180724_094554_0e19_3B', '20170920_015736_0f28_3B', '20180427_020503_103c_3B']\n",
      "20180419_074324_0c43_3B_0033.tif\n",
      "20180419_074324_0c43_3B_0033.tif\n",
      "20180419_074324_0c43_3B_0033.tif\n",
      "20180419_074324_1_0c43_3B_0112.tif\n",
      "20180419_074324_1_0c43_3B_0112.tif\n",
      "20180419_074324_1_0c43_3B_0112.tif\n",
      "20180419_074323_0c43_3B_0128.tif\n",
      "20180419_074323_0c43_3B_0128.tif\n",
      "20180419_074323_0c43_3B_0128.tif\n",
      "20180504_094435_0e19_3B_0236.tif\n",
      "20180504_094435_0e19_3B_0236.tif\n",
      "20180504_094435_0e19_3B_0236.tif\n",
      "20180419_074325_0c43_3B_0072.tif\n",
      "20180419_074325_0c43_3B_0072.tif\n",
      "20180419_074325_0c43_3B_0072.tif\n",
      "20180419_074325_0c43_3B_0184.tif\n",
      "20180419_074325_0c43_3B_0184.tif\n",
      "20180419_074325_0c43_3B_0184.tif\n",
      "20180504_094435_0e19_3B_0335.tif\n",
      "20180504_094435_0e19_3B_0335.tif\n",
      "20180504_094435_0e19_3B_0335.tif\n",
      "20180419_074324_0c43_3B_0026.tif\n",
      "20180419_074324_0c43_3B_0026.tif\n",
      "20180419_074324_0c43_3B_0026.tif\n",
      "20180724_094554_0e19_3B_0207.tif\n",
      "20180724_094554_0e19_3B_0207.tif\n",
      "20180724_094554_0e19_3B_0207.tif\n",
      "20180504_094435_0e19_3B_0161.tif\n",
      "20180504_094435_0e19_3B_0161.tif\n",
      "20180504_094435_0e19_3B_0161.tif\n",
      "20180724_094554_0e19_3B_0493.tif\n",
      "20180724_094554_0e19_3B_0493.tif\n",
      "20180724_094554_0e19_3B_0493.tif\n",
      "20180724_094554_0e19_3B_0303.tif\n",
      "20180724_094554_0e19_3B_0303.tif\n",
      "20180724_094554_0e19_3B_0303.tif\n",
      "20180724_094554_0e19_3B_0496.tif\n",
      "20180724_094554_0e19_3B_0496.tif\n",
      "20180724_094554_0e19_3B_0496.tif\n",
      "20180419_074323_0c43_3B_0135.tif\n",
      "20180419_074323_0c43_3B_0135.tif\n",
      "20180419_074323_0c43_3B_0135.tif\n",
      "20180419_074324_0c43_3B_0317.tif\n",
      "20180419_074324_0c43_3B_0317.tif\n",
      "20180419_074324_0c43_3B_0317.tif\n",
      "20180724_094554_0e19_3B_0608.tif\n",
      "20180724_094554_0e19_3B_0608.tif\n",
      "20180724_094554_0e19_3B_0608.tif\n",
      "20180504_094435_0e19_3B_0115.tif\n",
      "20180504_094435_0e19_3B_0115.tif\n",
      "20180504_094435_0e19_3B_0115.tif\n",
      "20180504_094435_0e19_3B_0238.tif\n",
      "20180504_094435_0e19_3B_0238.tif\n",
      "20180504_094435_0e19_3B_0238.tif\n",
      "20180504_094435_0e19_3B_0558.tif\n",
      "20180504_094435_0e19_3B_0558.tif\n",
      "20180504_094435_0e19_3B_0558.tif\n",
      "20180419_074324_1_0c43_3B_0079.tif\n",
      "20180419_074324_1_0c43_3B_0079.tif\n",
      "20180419_074324_1_0c43_3B_0079.tif\n",
      "20180419_074324_1_0c43_3B_0152.tif\n",
      "20180419_074324_1_0c43_3B_0152.tif\n",
      "20180419_074324_1_0c43_3B_0152.tif\n",
      "20180419_074325_0c43_3B_0070.tif\n",
      "20180419_074325_0c43_3B_0070.tif\n",
      "20180419_074325_0c43_3B_0070.tif\n",
      "20180419_074324_0c43_3B_0151.tif\n",
      "20180419_074324_0c43_3B_0151.tif\n",
      "20180419_074324_0c43_3B_0151.tif\n",
      "20180504_094435_0e19_3B_0470.tif\n",
      "20180504_094435_0e19_3B_0470.tif\n",
      "20180504_094435_0e19_3B_0470.tif\n",
      "20180724_094554_0e19_3B_0084.tif\n",
      "20180724_094554_0e19_3B_0084.tif\n",
      "20180724_094554_0e19_3B_0084.tif\n",
      "20180504_094435_0e19_3B_0253.tif\n",
      "20180504_094435_0e19_3B_0253.tif\n",
      "20180504_094435_0e19_3B_0253.tif\n",
      "20180724_094554_0e19_3B_0133.tif\n",
      "20180724_094554_0e19_3B_0133.tif\n",
      "20180724_094554_0e19_3B_0133.tif\n",
      "20180419_074325_0c43_3B_0257.tif\n",
      "20180419_074325_0c43_3B_0257.tif\n",
      "20180419_074325_0c43_3B_0257.tif\n",
      "20180504_094435_0e19_3B_0596.tif\n",
      "20180504_094435_0e19_3B_0596.tif\n",
      "20180504_094435_0e19_3B_0596.tif\n",
      "20180724_094554_0e19_3B_0601.tif\n",
      "20180724_094554_0e19_3B_0601.tif\n",
      "20180724_094554_0e19_3B_0601.tif\n",
      "20180419_074325_0c43_3B_0153.tif\n",
      "20180419_074325_0c43_3B_0153.tif\n",
      "20180419_074325_0c43_3B_0153.tif\n",
      "20180724_094554_0e19_3B_0190.tif\n",
      "20180724_094554_0e19_3B_0190.tif\n",
      "20180724_094554_0e19_3B_0190.tif\n",
      "20180724_094554_0e19_3B_0229.tif\n",
      "20180724_094554_0e19_3B_0229.tif\n",
      "20180724_094554_0e19_3B_0229.tif\n",
      "20180504_094435_0e19_3B_0416.tif\n",
      "20180504_094435_0e19_3B_0416.tif\n",
      "20180504_094435_0e19_3B_0416.tif\n",
      "20180504_094435_0e19_3B_0691.tif\n",
      "20180504_094435_0e19_3B_0691.tif\n",
      "20180504_094435_0e19_3B_0691.tif\n",
      "20180419_074323_0c43_3B_0158.tif\n",
      "20180419_074323_0c43_3B_0158.tif\n",
      "20180419_074323_0c43_3B_0158.tif\n",
      "20180419_074323_0c43_3B_0215.tif\n",
      "20180419_074323_0c43_3B_0215.tif\n",
      "20180419_074323_0c43_3B_0215.tif\n",
      "20180419_074323_0c43_3B_0129.tif\n",
      "20180419_074323_0c43_3B_0129.tif\n",
      "20180419_074323_0c43_3B_0129.tif\n",
      "20180419_074325_0c43_3B_0152.tif\n",
      "20180419_074325_0c43_3B_0152.tif\n",
      "20180419_074325_0c43_3B_0152.tif\n",
      "20180419_074325_0c43_3B_0183.tif\n",
      "20180419_074325_0c43_3B_0183.tif\n",
      "20180419_074325_0c43_3B_0183.tif\n",
      "20180724_094554_0e19_3B_0145.tif\n",
      "20180724_094554_0e19_3B_0145.tif\n",
      "20180724_094554_0e19_3B_0145.tif\n",
      "20180504_094435_0e19_3B_0141.tif\n",
      "20180504_094435_0e19_3B_0141.tif\n",
      "20180504_094435_0e19_3B_0141.tif\n",
      "20180419_074324_1_0c43_3B_0086.tif\n",
      "20180419_074324_1_0c43_3B_0086.tif\n",
      "20180419_074324_1_0c43_3B_0086.tif\n",
      "20180419_074324_0c43_3B_0267.tif\n",
      "20180419_074324_0c43_3B_0267.tif\n",
      "20180419_074324_0c43_3B_0267.tif\n",
      "20180724_094554_0e19_3B_0652.tif\n",
      "20180724_094554_0e19_3B_0652.tif\n",
      "20180724_094554_0e19_3B_0652.tif\n",
      "20180724_094554_0e19_3B_0315.tif\n",
      "20180724_094554_0e19_3B_0315.tif\n",
      "20180724_094554_0e19_3B_0315.tif\n",
      "20180419_074324_1_0c43_3B_0016.tif\n",
      "20180419_074324_1_0c43_3B_0016.tif\n",
      "20180419_074324_1_0c43_3B_0016.tif\n",
      "20180419_074326_0c43_3B_0244.tif\n",
      "20180419_074326_0c43_3B_0244.tif\n",
      "20180419_074326_0c43_3B_0244.tif\n",
      "20180419_074324_1_0c43_3B_0022.tif\n",
      "20180419_074324_1_0c43_3B_0022.tif\n",
      "20180419_074324_1_0c43_3B_0022.tif\n",
      "20180419_074326_0c43_3B_0090.tif\n",
      "20180419_074326_0c43_3B_0090.tif\n",
      "20180419_074326_0c43_3B_0090.tif\n",
      "20180419_074324_1_0c43_3B_0130.tif\n",
      "20180419_074324_1_0c43_3B_0130.tif\n",
      "20180419_074324_1_0c43_3B_0130.tif\n",
      "20180419_074325_0c43_3B_0295.tif\n",
      "20180419_074325_0c43_3B_0295.tif\n",
      "20180419_074325_0c43_3B_0295.tif\n",
      "20180504_094435_0e19_3B_0226.tif\n",
      "20180504_094435_0e19_3B_0226.tif\n",
      "20180504_094435_0e19_3B_0226.tif\n",
      "20180724_094554_0e19_3B_0624.tif\n",
      "20180724_094554_0e19_3B_0624.tif\n",
      "20180724_094554_0e19_3B_0624.tif\n",
      "20180419_074324_1_0c43_3B_0077.tif\n",
      "20180419_074324_1_0c43_3B_0077.tif\n",
      "20180419_074324_1_0c43_3B_0077.tif\n",
      "20180504_094435_0e19_3B_0352.tif\n",
      "20180504_094435_0e19_3B_0352.tif\n",
      "20180504_094435_0e19_3B_0352.tif\n",
      "20180504_094435_0e19_3B_0458.tif\n",
      "20180504_094435_0e19_3B_0458.tif\n",
      "20180504_094435_0e19_3B_0458.tif\n",
      "20180419_074324_0c43_3B_0130.tif\n",
      "20180419_074324_0c43_3B_0130.tif\n",
      "20180419_074324_0c43_3B_0130.tif\n",
      "20180419_074324_0c43_3B_0148.tif\n",
      "20180419_074324_0c43_3B_0148.tif\n",
      "20180419_074324_0c43_3B_0148.tif\n",
      "20180504_094435_0e19_3B_0455.tif\n",
      "20180504_094435_0e19_3B_0455.tif\n",
      "20180504_094435_0e19_3B_0455.tif\n",
      "20180419_074324_1_0c43_3B_0227.tif\n",
      "20180419_074324_1_0c43_3B_0227.tif\n",
      "20180419_074324_1_0c43_3B_0227.tif\n",
      "20180419_074324_1_0c43_3B_0297.tif\n",
      "20180419_074324_1_0c43_3B_0297.tif\n",
      "20180419_074324_1_0c43_3B_0297.tif\n",
      "20180504_094435_0e19_3B_0160.tif\n",
      "20180504_094435_0e19_3B_0160.tif\n",
      "20180504_094435_0e19_3B_0160.tif\n",
      "20180504_094435_0e19_3B_0480.tif\n",
      "20180504_094435_0e19_3B_0480.tif\n",
      "20180504_094435_0e19_3B_0480.tif\n",
      "20180419_074324_0c43_3B_0166.tif\n",
      "20180419_074324_0c43_3B_0166.tif\n",
      "20180419_074324_0c43_3B_0166.tif\n",
      "20180419_074324_0c43_3B_0053.tif\n",
      "20180419_074324_0c43_3B_0053.tif\n",
      "20180419_074324_0c43_3B_0053.tif\n",
      "20180419_074323_0c43_3B_0064.tif\n",
      "20180419_074323_0c43_3B_0064.tif\n",
      "20180419_074323_0c43_3B_0064.tif\n",
      "20180504_094435_0e19_3B_0260.tif\n",
      "20180504_094435_0e19_3B_0260.tif\n",
      "20180504_094435_0e19_3B_0260.tif\n",
      "20180724_094554_0e19_3B_0191.tif\n",
      "20180724_094554_0e19_3B_0191.tif\n",
      "20180724_094554_0e19_3B_0191.tif\n",
      "20180504_094435_0e19_3B_0166.tif\n",
      "20180504_094435_0e19_3B_0166.tif\n",
      "20180504_094435_0e19_3B_0166.tif\n",
      "20180504_094435_0e19_3B_0155.tif\n",
      "20180504_094435_0e19_3B_0155.tif\n",
      "20180504_094435_0e19_3B_0155.tif\n",
      "20180419_074323_0c43_3B_0076.tif\n",
      "20180419_074323_0c43_3B_0076.tif\n",
      "20180419_074323_0c43_3B_0076.tif\n",
      "20180419_074324_0c43_3B_0167.tif\n",
      "20180419_074324_0c43_3B_0167.tif\n",
      "20180419_074324_0c43_3B_0167.tif\n",
      "20180419_074326_0c43_3B_0138.tif\n",
      "20180419_074326_0c43_3B_0138.tif\n",
      "20180419_074326_0c43_3B_0138.tif\n",
      "20180419_074325_0c43_3B_0225.tif\n",
      "20180419_074325_0c43_3B_0225.tif\n",
      "20180419_074325_0c43_3B_0225.tif\n",
      "../../data/train/sat : 299\n",
      "../../data/train/map : 299\n",
      "../../data/train/sat_rgb : 299\n",
      "../../data/validate/sat : 75\n",
      "../../data/validate/map : 75\n",
      "../../data/validate/sat_rgb : 75\n",
      "../../data/test/sat : 94\n",
      "../../data/test/map : 94\n",
      "../../data/test/sat_rgb : 94\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if do_data_split:\n",
    "    def should_make_tiles_from(r_analytic_name):\n",
    "        is_analytic_tif = r_analytic_name.endswith(\n",
    "            ('AnalyticMS.tif', 'AnalyticMS_SR.tif', 'AnalyticMS.tiff', 'AnalyticMS_SR.tiff')\n",
    "        )\n",
    "        return is_analytic_tif \n",
    "\n",
    "    file_prefixes = [ get_tile_prefix(r_analytic.name) \n",
    "                      for r_analytic in Path(raw_images_path).iterdir()  \n",
    "                        if  should_make_tiles_from(r_analytic.name)\n",
    "                    ]\n",
    "    print(file_prefixes)\n",
    "                \n",
    "    # copy files to train dir\n",
    "    train_fnames = train.name.values\n",
    "    for fname in train_fnames:\n",
    "        for file_type in [\"sat\", \"map\", \"sat_rgb\"]:\n",
    "            src = os.path.join(original_dataset_dir, file_type, fname)\n",
    "            dest = os.path.join(train_dir, file_type, fname)\n",
    "            if(os.path.exists(src)):\n",
    "                shutil.copy(src, dest)\n",
    "\n",
    "    # copy files to validation dir\n",
    "    validation_fnames = valid.name.values\n",
    "    for fname in validation_fnames:\n",
    "        for file_type in [\"sat\", \"map\", \"sat_rgb\"]:\n",
    "            print(fname)\n",
    "            src = os.path.join(original_dataset_dir, file_type, fname)\n",
    "            dest = os.path.join(validation_dir, file_type, fname)\n",
    "            if(os.path.exists(src)):\n",
    "                shutil.copy(src, dest)\n",
    "    # copy files to test dir\n",
    "    test_fnames = test.name.values\n",
    "    for fname in test_fnames:\n",
    "        for file_type in [\"sat\", \"map\", \"sat_rgb\"]:\n",
    "            src = os.path.join(original_dataset_dir, file_type, fname)\n",
    "            dest = os.path.join(test_dir, file_type, fname)\n",
    "            if(os.path.exists(src)):\n",
    "                shutil.copy(src, dest)\n",
    "                \n",
    "    # print overview\n",
    "    for directory in dirs:\n",
    "        for file_type in [\"sat\", \"map\", \"sat_rgb\"]:\n",
    "            target = os.path.join(directory, file_type)\n",
    "            print(target, \":\", len(os.listdir(target)))\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ImageDataGenerators for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = trainGenerator(\n",
    "    batch_size,'../../data/train','sat','map',\n",
    "    data_gen_args,\n",
    "    save_to_dir = imgdatagen_dir,\n",
    "    image_color_mode=\"rgba\",\n",
    "    target_size=target_size,\n",
    "    flag_multi_class=True,\n",
    "    num_class=3\n",
    ")\n",
    "\n",
    "validation_gen = trainGenerator(\n",
    "    batch_size,'../../data/validate','sat','map',\n",
    "    data_gen_args, \n",
    "    save_to_dir = None, \n",
    "    image_color_mode=\"rgba\", \n",
    "    target_size=target_size, \n",
    "    flag_multi_class=True,\n",
    "    num_class=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model, compile, show summary, possibly load weights, define callbacks (including checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 299 images belonging to 1 classes.\n",
      "Found 299 images belonging to 1 classes.\n",
      "2 [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "nonzero 2210\n"
     ]
    }
   ],
   "source": [
    "le = next(train_gen)\n",
    "print(len(le), (le[1][0][..., 1]))\n",
    "print('nonzero', np.sum(le[1][0][..., 1] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(le[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/roaddetection/src/models/catmodel.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (1, 1), padding=\"same\", activation=\"relu\")`\n",
      "  conv10 = layers.Conv2D(nClasses, 1, 1, activation='relu',border_mode='same')(conv9)\n",
      "/home/ubuntu/roaddetection/src/models/catmodel.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"re..., inputs=Tensor(\"in...)`\n",
      "  model = Model(input=inputs, output=conv10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 2368        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 128 147584      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 512)  2359808     conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 512)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 512)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 1024) 4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 1024) 9438208     conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16, 16, 1024) 0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 1024) 0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 512)  2097664     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 1024) 0           dropout_1[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 512)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 256)  524544      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 512)  0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 256 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 128 131200      up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 256 0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 128 295040      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 128, 128 147584      conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 256, 256, 128 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 64) 32832       up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256, 256, 128 0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 2)  1154        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 256, 256, 3)  9           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 65536, 3)     0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 65536, 3)     0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 256, 256, 3)  0           activation_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 31,033,419\n",
      "Trainable params: 31,033,419\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = unet((256, 256, 4), 3)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=dice_loss(0.001, 0.05),\n",
    "              loss_weights=loss_weights,\n",
    "              metrics=metrics)\n",
    "model.summary()\n",
    "if (pretrained_model_fn):\n",
    "    model.load_weights(pretrained_model_fn)\n",
    "model_checkpoint = ModelCheckpoint(trained_model_fn, monitor='loss',verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65536.0\n",
      "Epoch 1/5\n",
      "Found 75 images belonging to 1 classes.\n",
      "Found 75 images belonging to 1 classes.\n",
      "298/299 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.9092 - auc_roc: 0.9263"
     ]
    }
   ],
   "source": [
    "class_weights = create_class_weight(labeldict)\n",
    "#{0: 1., 1: 25., 2: 75.}\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[model_checkpoint],\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.plot(history[\"acc\"], label=\"acc\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    plt.plot(history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history[\"auc_roc\"], label=\"auc_roc\")\n",
    "    plt.plot(history[\"val_auc_roc\"], label=\"val_auc_roc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testGene = testGenerator(\"../../data/test/sat\",target_size=(256, 256),as_gray=False)\n",
    "n = 0\n",
    "for img, name in testGene:\n",
    "    results = model.predict(img, batch_size=1)\n",
    "    saveResult(\"../../data/test/predict\", results, name, True, 3)\n",
    "    n += 1\n",
    "    if(n>300):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
