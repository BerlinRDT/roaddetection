Detection and classification of roads in satellite imagery of environmentally sensitive areas
=============================================================================================

Portfolio project at Data Science Retreat, Berlin | Microsoft AI for Earth-sponsored project
--------------------------------------------------------------------------------------------

In the 21st century, intense road construction in rural areas all over the world is anticipated. A substantial part of these activities will affect environmentally sensitive areas, occur in an unplanned manner, and remain off the charts due to the absence of efficient ways of monitoring road construction. Inspired by a recent appeal to the AI community ([Laurance, W. F. Road mapping needs AI experts. Nature 558, 30; 2018](https://www.nature.com/articles/d41586-018-05343-2)), we present a deep learning model designed for detecting roads in satellite imagery from rural, particularly tropical, areas. Road labels from selected areas of Southeastern Borneo were kindly provided by the group of Prof. Laurance (James Cook University, Australia), and were also retrieved from OpenStreetMaps and generated by us. Satellite imagery courtesy of Planet Labs, Inc. We also kindly acknowledge help from Pierre Ibisch and Monika Hoffmann, Eberswalde University for Sustainable Development.

The project was developed up to the working prototype level by Lisa Heße, Kiran Prakash and Harald Hentschke during a three-month intensive training at [Data Science Retreat, Berlin](https://datascienceretreat.com/) (Batch #15, June 18 - September 13, 2018). See [detailed summary](https://github.com/BerlinRDT/roaddetection/blob/master/reports/Road_detection_project_DSR_report.pdf).

In May 2019, an extended team (now also including Tim Sergio Gago, Matthias Bohner, and Erik Seiert) received an [AI for Earth](https://www.microsoft.com/en-us/ai/ai-for-earth) grant by Microsoft to further develop the project.


![example_prediction](/reports/figures/models_unet_borneo_and_harz_05_09_16_22_20180427_020503_103c_3B_0094_exc.png)

Project Organization
------------

    ├── LICENSE
    ├── Makefile           <- Makefile with commands like `make data` or `make train`
    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── external       <- Data from third party sources.
    │   ├── interim        <- Intermediate data that has been transformed.
    │   ├── processed      <- The final, canonical data sets for modeling.
    │   └── raw            <- The original, immutable data dump.
    │
    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                         the creator's initials, and a short `-` delimited description, e.g.
    │                         `1.0-jqp-initial-data-exploration`.
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── data           <- Scripts to download or generate data
    │   │   └── make_dataset.py
    │   │
    │   ├── features       <- Scripts to turn raw data into features for modeling
    │   │   └── build_features.py
    │   │
    │   ├── models         <- Scripts to train models and then use trained models to make
    │   │   │                 predictions
    │   │   ├── predict_model.py
    │   │   └── train_model.py
    │   │
    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations
    │       └── visualize.py
    │
    └── tox.ini            <- tox file with settings for running tox; see tox.testrun.org


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
