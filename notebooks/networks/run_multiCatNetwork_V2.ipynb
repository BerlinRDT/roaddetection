{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net for multi-class semantic segmentation\n",
    "\n",
    "First load all the libraries\n",
    "\n",
    "\n",
    "tensorboard --logdir=~/roaddetection/notebooks/networks/logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "#from keras.layers import merge\n",
    "from src.models.catdata import *\n",
    "from src.models.catmodel import *\n",
    "from src.models.catsegnet import *\n",
    "from src.models.catloss import *\n",
    "\n",
    "from src.data.utils import get_tile_prefix\n",
    "from src.models.metrics_img import auc_roc, auc_pr, auc_pr_multiclass, dummy_metric\n",
    "#import rasterio.plot as rioplot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "\n",
    "from pathlib import Path\n",
    "import os, shutil\n",
    "import sys\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.callbacks import RemoteMonitor, TensorBoard, ReduceLROnPlateau, EarlyStopping, History\n",
    "from keras import backend as keras\n",
    "\n",
    "sys.path.append(\"/home/ubuntu/roaddetection/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the train, validation and test data sets\n",
    "\n",
    "To increase the U-Nets learning capacity, it is helpful to **first** train on high road-pixel density data.  This is achived by creating the corresponding image-sets according to the requirements.  \n",
    "\n",
    "In a **second** training step all the images, e.g. also low and even zero road-pixel density data is considered.  \n",
    "\n",
    "### Define the paths where the data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/train\n"
     ]
    }
   ],
   "source": [
    "original_dataset_dir = \"../../data/train_raw\"\n",
    "raw_images_path = \"../../data/raw/images\"\n",
    "dirs = []\n",
    "\n",
    "base_dir = \"../../data\"\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "dirs.append(train_dir)\n",
    "validation_dir = os.path.join(base_dir, \"validate\")\n",
    "dirs.append(validation_dir)\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "dirs.append(test_dir)\n",
    "\n",
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the directories if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in dirs:\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function which takes care of the assignement of image names to one of the three sets accoring to the desired pixel density of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5\n",
    "\n",
    "def selectData(tNoRoad = 0.95, tPRoad = 0, tURoad = 0, tEmpty = 0):\n",
    "    df = pd.read_csv('../visualize_imagery/numOfPixPerClassPerTile_256.csv').drop(['Unnamed: 0'], axis = 1)\n",
    "    #df = df[(df['region'] == 'Borneo')]\n",
    "    any_RP = df[((df['relative_noRoad']) < tNoRoad) \n",
    "             & ((df['relative_pavedRoad']) > 0)\n",
    "             & ((df['relative_unpavedRoad']) > 0)]\n",
    "    \n",
    "    unpaved_RP = df[((df['relative_noRoad']) < tNoRoad) \n",
    "                 & ((df['relative_pavedRoad']) == 0) \n",
    "                 & ((df['relative_unpavedRoad']) > tURoad)]\n",
    "    \n",
    "    paved_RP = df[((df['relative_noRoad']) < tNoRoad) \n",
    "                & ((df['relative_pavedRoad']) > tPRoad) \n",
    "                & ((df['relative_unpavedRoad']) == 0)]\n",
    "    \n",
    "    no_RP = df[(df['relative_noRoad'] == 1)].sample(frac=tEmpty, random_state=r)\n",
    "\n",
    "    assert len(any_RP.merge(unpaved_RP)) == 0\n",
    "    assert len(unpaved_RP.merge(paved_RP)) == 0\n",
    "    assert len(paved_RP.merge(no_RP)) == 0\n",
    "    assert len(no_RP.merge(any_RP)) == 0\n",
    "    \n",
    "    sdf = any_RP\n",
    "    \n",
    "    if tEmpty > 0:\n",
    "        sdf = pd.concat([sdf, no_RP])\n",
    "    if tPRoad > 0:\n",
    "        sdf = pd.concat([sdf, paved_RP])\n",
    "    if tURoad > 0:\n",
    "        sdf = pd.concat([sdf, unpaved_RP])\n",
    "    \n",
    "    sdf = shuffle(sdf, random_state=r).sample(frac=.4, random_state=r).reset_index(drop=True)\n",
    "\n",
    "    train_tmp, test = train_test_split(sdf, test_size=0.2, random_state=r)\n",
    "    train, valid = train_test_split(train_tmp, test_size=0.2, random_state=r)\n",
    "    return train, valid, test   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataframes assigning the image names to the corresponding classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = selectData(tNoRoad = .95)#tURoad = 0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Harz      225\n",
      "Borneo     13\n",
      "Name: region, dtype: int64\n",
      "\n",
      "Validation set\n",
      "Harz      58\n",
      "Borneo     2\n",
      "Name: region, dtype: int64\n",
      "\n",
      "Test set\n",
      "Harz      74\n",
      "Borneo     1\n",
      "Name: region, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set\")\n",
    "print(train.region.value_counts())\n",
    "print(\"\")\n",
    "print(\"Validation set\")\n",
    "print(valid.region.value_counts())\n",
    "print(\"\")\n",
    "print(\"Test set\")\n",
    "print(test.region.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function which copies the data according to the three dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_make_tiles_from(r_analytic_name):\n",
    "    is_analytic_tif = r_analytic_name.endswith(\n",
    "        ('AnalyticMS.tif', 'AnalyticMS_SR.tif', 'AnalyticMS.tiff', 'AnalyticMS_SR.tiff')\n",
    "    )\n",
    "    return is_analytic_tif \n",
    "\n",
    "def copy(fnames, src_dir):\n",
    "    for name in fnames:\n",
    "        for file_type in [\"sat\", \"map\", \"sat_rgb\"]:\n",
    "            src = os.path.join(original_dataset_dir, file_type, name)\n",
    "            dest = os.path.join(src_dir, file_type, name)\n",
    "            if(os.path.exists(src)):\n",
    "                shutil.copy(src, dest)\n",
    "\n",
    "def make_datasets(show_progress = False):\n",
    "    file_prefixes = [ get_tile_prefix(r_analytic.name) \n",
    "                      for r_analytic in Path(raw_images_path).iterdir()  \n",
    "                        if  should_make_tiles_from(r_analytic.name)\n",
    "                    ]\n",
    "\n",
    "    # copy files to train dir\n",
    "    train_fnames = train.name.values\n",
    "    copy(train_fnames, train_dir)\n",
    "    print(\"Create train data.\")\n",
    "\n",
    "    # copy files to validation dir\n",
    "    validation_fnames = valid.name.values\n",
    "    copy(validation_fnames, validation_dir)\n",
    "    print(\"Create validation data.\")\n",
    "\n",
    "    \n",
    "    # copy files to test dir\n",
    "    test_fnames = test.name.values\n",
    "    copy(test_fnames, test_dir)\n",
    "    print(\"Create test data.\")\n",
    "\n",
    "    # print overview\n",
    "    if show_progress == True:\n",
    "        for directory in dirs:\n",
    "            for file_type in [\"sat\", \"map\", \"sat_rgb\"]:\n",
    "                target = os.path.join(directory, file_type)\n",
    "                print(target, \":\", len(os.listdir(target)))\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create train data.\n",
      "Create validation data.\n",
      "Create test data.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#make_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create class weights accoriding to their train statistics\n",
    "\n",
    "This is recommended as the classes are imbalanced (more no-road pixels than road pixels in each image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict = {\n",
    "    0: train.numPixel_noRoad.mean(),\n",
    "    1: train.numPixel_pavedRoad.mean(),\n",
    "    2: train.numPixel_unpavedRoad.mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_weight(labels_dict,mu=0.25):\n",
    "    total = sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "    weights = []\n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "        weights.append(score if score > 1.0 else 1.0)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the data generator to flow the images from the directory while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, data_gen_args, data_dir, imgdatagen_dir, target_size):\n",
    "    return trainGenerator(\n",
    "            batch_size,\n",
    "            data_dir,\n",
    "            'sat',\n",
    "            'map',\n",
    "            data_gen_args,\n",
    "            save_to_dir = imgdatagen_dir,\n",
    "            image_color_mode=\"rgba\",\n",
    "            target_size=target_size,\n",
    "            flag_multi_class=True,\n",
    "            num_class=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the names of the models to save them on the hard drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_name(model, th0, th1, th2, th3, batch_size, epochs):\n",
    "    MODELDIR = '../../models/'\n",
    "    \n",
    "    mname = 'neuneu_multicat_' + model + '_NL_th0-' + str(th0) \\\n",
    "            + '_th1-' + str(th1)   \\\n",
    "            + '_th2-' + str(th2)   \\\n",
    "            + '_th3-' + str(th3) + '_bs-' + str(batch_size) \\\n",
    "            + '_ep-' + str(epochs)\n",
    "    \n",
    "    versions = []\n",
    "    for file in Path(MODELDIR).iterdir():\n",
    "        if file.name.startswith((mname)):\n",
    "            versions.append(int(file.name.rsplit(mname+'_r-')[1].split('.')[0]))\n",
    "    latest = 1\n",
    "    if len(versions) > 0:\n",
    "        latest = np.max(versions) + 1\n",
    "    model_name =  MODELDIR + mname + '_r-' + str(latest) + '.hdf5'\n",
    "    pretrained = ''\n",
    "    if(latest > 1):\n",
    "        pretrained =  MODELDIR + mname + '_r-' + str(latest-1) + '.hdf5'\n",
    "    return model_name, pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history[\"acc\"], label=\"acc\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    plt.plot(history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()#\n",
    "    plt.plot(history[\"auc_pr_multiclass\"], label=\"auc_pr_multiclass\")\n",
    "    plt.plot(history[\"val_auc_pr_multiclass\"], label=\"val_auc_pr_multiclass\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model):\n",
    "    testGene = testGenerator(\"../../data/test/sat\",target_size=(256, 256),as_gray=False)\n",
    "    n = 0\n",
    "    for img, name in testGene:\n",
    "        results = model.predict(img, batch_size=1)\n",
    "        saveResult(\"../../data/test/predict\", results, name, True, 3)\n",
    "        n += 1\n",
    "        if(n>300):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the training schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go(model = 'unet', th0 = 0.01, th1 = 0.08, th2=0.03, th3 = 0.04, target_size = (256, 256), batch_size = 3, epochs = 5, data_aug = True, pretrained = False):\n",
    "    \n",
    "    valid_gen_args = dict(data_format=\"channels_last\")\n",
    "    \n",
    "    if (data_aug == True):\n",
    "        train_gen_args = dict(\n",
    "            data_format=\"channels_last\",\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True\n",
    "        )\n",
    "    else:\n",
    "        train_gen_args = valid_gen_args\n",
    "    \n",
    "    trained_model_fn, pretrained_model_fn = model_name(model, th0, th1, th2, th3, batch_size, epochs)\n",
    "         \n",
    "    steps_per_epoch  = len(train) // batch_size\n",
    "    validation_steps = len(valid) // batch_size    \n",
    "    \n",
    "    optimizer    = Adam(lr=1e-4)\n",
    "    loss_weights = None\n",
    "    metrics      = ['accuracy', auc_pr_multiclass]\n",
    "    \n",
    "    imgdatagen_dir = None    \n",
    "        \n",
    "    train_gen      = data_generator(batch_size, train_gen_args, train_dir, imgdatagen_dir, target_size)    \n",
    "    validation_gen = data_generator(batch_size, valid_gen_args, validation_dir, imgdatagen_dir,target_size)    \n",
    "    \n",
    "    width, height = target_size\n",
    "    if model == 'unet':\n",
    "        model = munet((width, height, 4), 3)\n",
    "    if model == 'segnet':\n",
    "        model = segnet((width, height, 4), 3)\n",
    "        \n",
    "        \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=noisy_loss(th0, th1, th2, th3),\n",
    "                  loss_weights=loss_weights,\n",
    "                  metrics=metrics)\n",
    "    #model.summary()\n",
    "    \n",
    "    if (pretrained == True):\n",
    "        if (len(pretrained_model_fn) > 0):\n",
    "            print('... loading the pretrained model', pretrained_model_fn)\n",
    "            model.load_weights(pretrained_model_fn)\n",
    "        else:\n",
    "            print('load the weights from the pretrained binary model...')\n",
    "            binary_model = load_model('../../models/unet_borneo_and_harz_05_09_16_22.hdf5', custom_objects={'auc_pr': auc_pr})\n",
    "\n",
    "            for layer, pretrained_layer in zip(model.layers[1:38], binary_model.layers[1:38]):\n",
    "                layer.set_weights(pretrained_layer.get_weights())\n",
    "        #else:\n",
    "        #    print('load default model')\n",
    "        #   # model.load_weights('../../models/0905_1500_NL_multiCat_unet.hdf5')#multicat_unet_NL_th0-0.01_th1-0.01_bs-3_ep-18_r-3.hdf5')#multicat_unet_NL_th0-0.08_th1-0.08_bs-3_ep-10_r-2.hdf5')\n",
    "        #    model.load_weights('../../models/multicat_unet_NL_th0-0.01_th1-0.08_bs-3_ep-30_r-2.hdf5')#multicat_unet_NL_th0-0.08_th1-0.04_bs-3_ep-10_r-1.hdf5')#../../models/multicat_unet_NL_th0-0.01_th1-0.08_bs-3_ep-5_r-6.hdf5')\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(trained_model_fn, monitor='loss',verbose=1, save_best_only=True)\n",
    "    leaning_rate     = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    early_stop       = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=1, mode='auto', baseline=None)\n",
    "    hist             = History()\n",
    "    tesorboard = TrainValTensorBoard(log_dir='./logs', batch_size=batch_size, \n",
    "                             write_graph=True, \n",
    "                             write_grads=True, \n",
    "                             write_images=True)\n",
    "\n",
    "    class_weights = create_class_weight(labeldict)\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[model_checkpoint, leaning_rate, early_stop, hist, tesorboard],\n",
    "        validation_data=validation_gen,\n",
    "        validation_steps=validation_steps\n",
    "    )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the TensorBoard output such that training and validation metrics and losses are shown in one graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model and create the prediction afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/roaddetection/src/models/catmodel.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (1, 1), padding=\"same\", activation=\"relu\")`\n",
      "  \n",
      "/home/ubuntu/roaddetection/src/models/catmodel.py:69: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"re..., inputs=Tensor(\"in...)`\n",
      "  conv10 = layers.core.Reshape((input_size[0], input_size[1], nClasses))(conv10)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "noisy_loss() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d3d8749b8ffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                     \u001b[0mglob_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-a284876df3e5>\u001b[0m in \u001b[0;36mgo\u001b[0;34m(model, th0, th1, th2, th3, target_size, batch_size, epochs, data_aug, pretrained)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     model.compile(optimizer=optimizer,\n\u001b[0;32m---> 36\u001b[0;31m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoisy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                   \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                   metrics=metrics)\n",
      "\u001b[0;31mTypeError\u001b[0m: noisy_loss() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "glob_history = {}\n",
    "for t0 in [0.0]:\n",
    "    for t1 in [0.0]:\n",
    "        for t2 in [0.0]:\n",
    "            for t3 in [0.0]:\n",
    "                for i in range(3):\n",
    "                    history, model = go(batch_size = 3, epochs = 30, pretrained = True, th0=t0, th1=t1, th2=t2, th3=t3)\n",
    "                    glob_history.update(history.history)\n",
    "                    plot_history(glob_history)\n",
    "                    prediction(model)\n",
    "                    del history\n",
    "                    del model\n",
    "                    keras.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../../models/neuneu*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
