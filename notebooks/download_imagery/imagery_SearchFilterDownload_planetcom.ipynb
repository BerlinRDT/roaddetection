{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search, filter and download satellite imagery from planet.com\n",
    "\n",
    "This notebook is a mishmash of several notebooks, most of them provided by planet.com on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import planet as pl\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# TODO / improvements:\n",
    "# - filter images according to overlap with area of interest (AOI)\n",
    "# - read geojson files instead of specifying coordinates in code below for AOI\n",
    "# - deal with time interval in which to retrieve data programmatically, e.g.\n",
    "#   up to 3 months before last road label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planet.com API setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read API Key stored as an env variable\n",
    "PLANET_API_KEY = os.getenv('PL_API_KEY')\n",
    "if PLANET_API_KEY is None:\n",
    "    raise Exception(\"key does not exist\")\n",
    "\n",
    "# Setup Planet Data API base URL:\n",
    "URL = \"https://api.planet.com/data/v1\"\n",
    "\n",
    "# - Setup the session\n",
    "session = requests.Session()\n",
    "\n",
    "# - Authenticate\n",
    "session.auth = (PLANET_API_KEY, \"\")\n",
    "\n",
    "# - Make a GET request to the Planet Data API\n",
    "res = session.get(URL)\n",
    "\n",
    "if res.status_code != 200:\n",
    "    session.close()\n",
    "    raise Exception(\"Houston, we have no planet\")\n",
    "\n",
    "# - Setup the quick search endpoint url (used for temporary searches as this one)\n",
    "quick_url = \"{}/quick-search\".format(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined parameters for imagery to be retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all 'global' parameters needed to let this notebook do its job without user interaction\n",
    "\n",
    "# if true, lots of information will be displayed\n",
    "be_verbose = False\n",
    "# if true, assets will neither be activated nor downloaded\n",
    "dry_run = False\n",
    "\n",
    "# - name of area of interest\n",
    "aoi = \"3093\"\n",
    "aoi = \"3347\"\n",
    "\n",
    "# - directory to dump imagery into\n",
    "data_dir = \"/media/hh/hd_internal/_data_DS/DSR/satelliteImages/Borneo/\" + aoi + \"/\"\n",
    "\n",
    "# - maximal fraction of cloud cover\n",
    "cloud_cover_max = 0.01 \n",
    "\n",
    "# - minimal sun elevation: make sure it's not set too high and chimes with the season (date range)\n",
    "sun_elevation_min = 45\n",
    "\n",
    "# - type of product\n",
    "# PSScene3Band - PlanetScope 3-band Basic and Ortho Scenes\n",
    "# PSScene4Band - PlanetScope 4-band Basic and Ortho Scenes **\n",
    "# PSOrthoTile - PlanetScope 4-band Ortho Tiles as 25 km x 25 km UTM tiles\n",
    "# SkySatScene - SkySat Basic and Ortho Scenes\n",
    "# SkySatCollect -  is created by composing SkySat Ortho Scenes\n",
    "item_type = \"PSScene4Band\"\n",
    "\n",
    "# - asset types (see https://api.planet.com/data/v1/asset-types/)\n",
    "asset_type = (\n",
    "    \"analytic\", # Radiometrically-calibrated analytic imagery stored as 16-bit scaled radiance, suitable for analytic applications.\n",
    "    \"analytic_sr\", #Atmospherically-corrected analytic imagery stored as 16-bit scaled (surface) reflectance, suitable for analytic applications.\n",
    "    \"analytic_xml\", # Radiometrically-calibrated analytic image metadata\n",
    "    \"ortho_analytic_dn\", # Orthorectified 16-bit 4-Band DN Image\n",
    "    \"ortho_analytic_udm\", # Orthorectified 16-bit 4-Band DN Image Unuseable Data Mask\n",
    "    \"ortho_visual,\"\n",
    "    \"visual\",\n",
    "    \"visual_xml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descript</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>date_label_min</th>\n",
       "      <th>date_label_max</th>\n",
       "      <th>date_retrieve_min</th>\n",
       "      <th>date_retrieve_max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>Borneo, labels by Laurance group</td>\n",
       "      <td>[[[115.7397882465956, -0.8704379025218731], [1...</td>\n",
       "      <td>n.d.</td>\n",
       "      <td>n.d.</td>\n",
       "      <td>2018-01-01T00:00:00.000Z</td>\n",
       "      <td>2018-06-30T00:00:00.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              descript  \\\n",
       "name                                     \n",
       "3347  Borneo, labels by Laurance group   \n",
       "\n",
       "                                            coordinates date_label_min  \\\n",
       "name                                                                     \n",
       "3347  [[[115.7397882465956, -0.8704379025218731], [1...           n.d.   \n",
       "\n",
       "     date_label_max         date_retrieve_min         date_retrieve_max  \n",
       "name                                                                     \n",
       "3347           n.d.  2018-01-01T00:00:00.000Z  2018-06-30T00:00:00.000Z  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define list of AOI \n",
    "\n",
    "dict_aoi = {\n",
    "    \"name\": \"3093\",     # keys correspond to numeric code of Laurance lab (e.g. 3093)\n",
    "    \"descript\": \"Borneo, labels by Laurance group\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [ \n",
    "            [115.7397189289545,-1.680836871258456],\n",
    "            [116.2810919527378,-1.680331498795499],\n",
    "            [116.2805050935049,-1.229806405260338],\n",
    "            [115.7397803420542,-1.230358513985345],\n",
    "            [115.7397189289545,-1.680836871258456]      \n",
    "            ]\n",
    "        ]\n",
    "    ],\n",
    "    \"date_label_min\" : \"2005-11-08T00:00:00.000Z\", # earliest date of labeled road\n",
    "    \"date_label_max\" : \"2017-12-31T00:00:00.000Z\", # latest date of labeled road\n",
    "    \"date_retrieve_min\" : \"2017-06-01T00:00:00.000Z\", # earliest date of imagery to be retrieved\n",
    "    \"date_retrieve_max\" : \"2017-12-31T00:00:00.000Z\", # latest date of imagery to be retrieved\n",
    "}\n",
    "\n",
    "dict_aoi = {\n",
    "    \"name\": \"3347\",     # keys correspond to numeric code of Laurance lab (e.g. 3093)\n",
    "    \"descript\": \"Borneo, labels by Laurance group\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [ \n",
    "            [115.7397882465956,-0.8704379025218731],\n",
    "            [116.2791973232524,-0.8694242846142544],\n",
    "            [116.2802063071478,-0.4201723315739193],\n",
    "            [115.73981363248,-0.4205599493179044],\n",
    "            [115.7397882465956,-0.8704379025218731]      \n",
    "            ]\n",
    "        ]\n",
    "    ],\n",
    "    \"date_label_min\" : \"n.d.\", # earliest date of labeled road\n",
    "    \"date_label_max\" : \"n.d.\", # latest date of labeled road\n",
    "    \"date_retrieve_min\" : \"2018-01-01T00:00:00.000Z\", # earliest date of imagery to be retrieved\n",
    "    \"date_retrieve_max\" : \"2018-06-30T00:00:00.000Z\", # latest date of imagery to be retrieved\n",
    "}\n",
    "\n",
    "# convert to dataframe so we can take a look\n",
    "df_aoi = pd.DataFrame(dict_aoi).set_index(\"name\")\n",
    "df_aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load coordinates into geojson_geometry\n",
    "geojson_geometry = {\n",
    "  \"type\": \"Polygon\",\n",
    "  \"coordinates\": df_aoi.loc[aoi,\"coordinates\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# Helper function to print formatted JSON using the json module\n",
    "def p(data):\n",
    "    print(json.dumps(data, indent=2))\n",
    "\n",
    "# Function to download asset files\n",
    "def download_ass(url, data_dir=\"~/Downloads/\", filename=None):\n",
    "    \"\"\"Downloads asset files from given site; input variables:\n",
    "       - url (the location url)\n",
    "       - data_dir - directory (default: ~/Downloads/)\n",
    "       - filename (the filename to save it as. defaults to whatever the file is called originally)\n",
    "    \"\"\"\n",
    "    # Send a GET request to the provided location url, using API Key for authentication\n",
    "    res = requests.get(url, stream=True, auth=(PLANET_API_KEY, \"\"))\n",
    "    # If no filename argument is given\n",
    "    if not filename:\n",
    "        # Construct a filename from the API response\n",
    "        if \"content-disposition\" in res.headers:\n",
    "            filename = res.headers[\"content-disposition\"].split(\"filename=\")[-1].strip(\"'\\\"\")\n",
    "        # Construct a filename from the location url\n",
    "        else:\n",
    "            filename = url.split(\"=\")[1][:10]\n",
    "    # Save the file\n",
    "    with open(data_dir + filename, \"wb\") as f:\n",
    "        for chunk in res.iter_content(chunk_size=1024):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "                f.flush()\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images that overlap with our AOI \n",
    "geometry_filter = {\n",
    "  \"type\": \"GeometryFilter\",\n",
    "  \"field_name\": \"geometry\",\n",
    "  \"config\": geojson_geometry\n",
    "}\n",
    "\n",
    "# - date range\n",
    "date_range_filter = {\n",
    "  \"type\": \"DateRangeFilter\",\n",
    "  \"field_name\": \"acquired\",\n",
    "  \"config\": {\n",
    "    \"gte\": df_aoi.loc[aoi,\"date_retrieve_min\"], # greater than or equal to\n",
    "    \"lte\": df_aoi.loc[aoi,\"date_retrieve_max\"]  # less than or equal to\n",
    "  }\n",
    "}\n",
    "\n",
    "# - cloud coverage\n",
    "cloud_cover_filter = {\n",
    "  \"type\": \"RangeFilter\",\n",
    "  \"field_name\": \"cloud_cover\",\n",
    "  \"config\": {\n",
    "    \"lte\": cloud_cover_max\n",
    "  }\n",
    "}\n",
    "\n",
    "# - sun elevation\n",
    "sun_elevation_filter = {\n",
    "  \"type\": \"RangeFilter\",\n",
    "  \"field_name\": \"sun_elevation\",\n",
    "  \"config\": {\n",
    "    \"gte\": sun_elevation_min\n",
    "  }\n",
    "}\n",
    "\n",
    "# combine filters\n",
    "combined_filter = {\n",
    "  \"type\": \"AndFilter\",\n",
    "  \"config\": [geometry_filter, date_range_filter, cloud_cover_filter, sun_elevation_filter]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching: Items and Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 38 items\n"
     ]
    }
   ],
   "source": [
    "# API request object\n",
    "search_request = {\n",
    "  \"interval\": \"day\",\n",
    "  \"item_types\": [item_type], \n",
    "  \"filter\": combined_filter\n",
    "}\n",
    "\n",
    "# Send the POST request to the API quick search endpoint\n",
    "search_result = session.post(quick_url, json=search_request)\n",
    "\n",
    "# srj is a nested dict with the following keys at the top level:\n",
    "# \"_links\" \n",
    "# \"features\" - a list of the retrieved items\n",
    "# \"type\"\n",
    "srj = search_result.json()\n",
    "if be_verbose:\n",
    "    # pretty-print\n",
    "    p(srj)\n",
    "\n",
    "print(\"found \" + str(len(srj[\"features\"])) + \" items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"202\": 99,\n",
      "  \"204\": 12,\n",
      "  \"401\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# loop over features and their assets to activate\n",
    "status_count = {202 : 0, # The request has been accepted and the activation will begin shortly. \n",
    "                204: 0, # The asset is already active and no further action is needed. \n",
    "                401: 0 # The user does not have permissions to download this file.\n",
    "                }\n",
    "assets_get_list = []\n",
    "for feature in srj[\"features\"]:\n",
    "    assets_url = feature[\"_links\"][\"assets\"]\n",
    "    res = session.get(assets_url)\n",
    "    # Assign a variable to the item's assets url response\n",
    "    assets = res.json()\n",
    "    if be_verbose:\n",
    "        print(\"available assets: \" + str(assets.keys()))\n",
    "    # set of assets which are available and desired \n",
    "    assets_available = set(assets.keys()).intersection(set(asset_type))\n",
    "    if be_verbose:\n",
    "        print(\"available and requested assets: \" + str(assets_available))    \n",
    "    # list of assets for which we have permission to download\n",
    "    assets_permitted = [key for key in assets.keys() if (assets[key][\"_permissions\"][0] == \"download\")]\n",
    "    # intersection of both\n",
    "    assets_get = assets_available.intersection(set(assets_permitted))\n",
    "    if be_verbose:\n",
    "        print(\"available, requested and permitted assets: \" + str(assets_get))\n",
    "    # put in list\n",
    "    assets_get_list.append(assets_get)\n",
    "    # if list is not empty:\n",
    "    if (assets_available and not dry_run):\n",
    "        for ass in assets_get:\n",
    "            activation_url = assets[ass][\"_links\"][\"activate\"]\n",
    "            # Send a request to the activation url to activate the item\n",
    "            res_activation = session.get(activation_url)\n",
    "            # update status count\n",
    "            status_count[res_activation.status_code] += 1\n",
    "            # update of list of items to get - unfisnished, not working\n",
    "            #if (res_activation.status_code in [202, 204]):\n",
    "            #    assets_get_list[fCount].append(ass)\n",
    "            \n",
    "p(status_count)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180622_020715_103d: downloading analytic...\n",
      "20180622_020715_103d: downloading analytic_xml...\n",
      "20180618_020845_0f42: downloading analytic...\n",
      "20180618_020845_0f42: downloading analytic_xml...\n",
      "20180618_020842_0f42: downloading analytic...\n",
      "20180618_020842_0f42: downloading analytic_sr...\n",
      "20180618_020842_0f42: downloading analytic_xml...\n",
      "20180618_020841_0f42: downloading analytic...\n",
      "20180618_020841_0f42: downloading analytic_sr...\n",
      "20180618_020841_0f42: downloading analytic_xml...\n",
      "20180618_020840_0f42: downloading analytic...\n",
      "20180618_020840_0f42: downloading analytic_sr...\n",
      "20180618_020840_0f42: downloading analytic_xml...\n",
      "20180618_020844_0f42: downloading analytic...\n",
      "20180618_020844_0f42: downloading analytic_sr...\n",
      "20180618_020844_0f42: downloading analytic_xml...\n",
      "20180618_020524_1033: downloading analytic...\n",
      "20180618_020524_1033: downloading analytic_sr...\n",
      "20180618_020524_1033: downloading analytic_xml...\n",
      "20180618_020523_1033: downloading analytic...\n",
      "20180618_020523_1033: downloading analytic_sr...\n",
      "20180618_020523_1033: downloading analytic_xml...\n",
      "20180618_020522_1033: downloading analytic...\n",
      "20180618_020522_1033: downloading analytic_sr...\n",
      "20180618_020522_1033: downloading analytic_xml...\n",
      "20180618_020348_0e26: downloading analytic...\n",
      "20180618_020348_0e26: downloading analytic_sr...\n",
      "20180618_020348_0e26: downloading analytic_xml...\n",
      "20180618_020347_0e26: downloading analytic...\n",
      "20180618_020347_0e26: downloading analytic_sr...\n",
      "20180618_020347_0e26: downloading analytic_xml...\n",
      "20180618_020346_0e26: downloading analytic...\n",
      "20180618_020346_0e26: downloading analytic_sr...\n",
      "20180618_020346_0e26: downloading analytic_xml...\n",
      "20180618_020345_0e26: downloading analytic...\n",
      "20180618_020345_0e26: downloading analytic_sr...\n",
      "20180618_020345_0e26: downloading analytic_xml...\n",
      "20180618_025312_1020: downloading analytic...\n",
      "20180618_025312_1020: downloading analytic_sr...\n",
      "20180618_025312_1020: downloading analytic_xml...\n",
      "20180618_025311_1020: downloading analytic...\n",
      "20180618_025311_1020: downloading analytic_sr...\n",
      "20180618_025311_1020: downloading analytic_xml...\n",
      "20180505_020205_0c38: downloading analytic...\n",
      "20180505_020205_0c38: downloading analytic_xml...\n",
      "20180503_020358_100c: downloading analytic...\n",
      "20180503_020358_100c: downloading analytic_sr...\n",
      "20180503_020358_100c: downloading analytic_xml...\n",
      "20180427_030120_0f1a: downloading analytic...\n",
      "20180427_030120_0f1a: downloading analytic_sr...\n",
      "20180427_030120_0f1a: downloading analytic_xml...\n",
      "20180323_020159_1040: downloading analytic...\n",
      "20180323_020159_1040: downloading analytic_sr...\n",
      "20180323_020159_1040: downloading analytic_xml...\n",
      "20180312_020340_0e0f: downloading analytic...\n",
      "20180312_020340_0e0f: downloading analytic_sr...\n",
      "20180312_020340_0e0f: downloading analytic_xml...\n",
      "20180312_020339_0e0f: downloading analytic...\n",
      "20180312_020339_0e0f: downloading analytic_sr...\n",
      "20180312_020339_0e0f: downloading analytic_xml...\n",
      "20180311_020411_0f51: downloading analytic...\n",
      "20180311_020411_0f51: downloading analytic_sr...\n",
      "20180311_020411_0f51: downloading analytic_xml...\n",
      "20180311_020410_0f51: downloading analytic...\n",
      "20180311_020410_0f51: downloading analytic_sr...\n",
      "20180311_020410_0f51: downloading analytic_xml...\n",
      "20180311_020128_103d: downloading analytic...\n",
      "20180311_020128_103d: downloading analytic_sr...\n",
      "20180311_020128_103d: downloading analytic_xml...\n",
      "20180311_020412_0f51: downloading analytic...\n",
      "20180311_020412_0f51: downloading analytic_sr...\n",
      "20180311_020412_0f51: downloading analytic_xml...\n",
      "20180311_020126_103d: downloading analytic...\n",
      "20180311_020126_103d: downloading analytic_sr...\n",
      "20180311_020126_103d: downloading analytic_xml...\n",
      "20180311_020125_103d: downloading analytic...\n",
      "20180311_020125_103d: downloading analytic_sr...\n",
      "20180311_020125_103d: downloading analytic_xml...\n",
      "20180311_020130_103d: downloading analytic...\n",
      "20180311_020130_103d: downloading analytic_sr...\n",
      "20180311_020130_103d: downloading analytic_xml...\n",
      "20180311_020129_103d: downloading analytic...\n",
      "20180311_020129_103d: downloading analytic_sr...\n",
      "20180311_020129_103d: downloading analytic_xml...\n",
      "20180311_020127_103d: downloading analytic...\n",
      "20180311_020127_103d: downloading analytic_sr...\n",
      "20180311_020127_103d: downloading analytic_xml...\n",
      "20180311_020131_103d: downloading analytic...\n",
      "20180311_020131_103d: downloading analytic_sr...\n",
      "20180311_020131_103d: downloading analytic_xml...\n",
      "20180224_030924_0f47: downloading analytic...\n",
      "20180224_030924_0f47: downloading analytic_sr...\n",
      "20180224_030924_0f47: downloading analytic_xml...\n",
      "20180203_020044_1034: downloading analytic...\n",
      "20180203_020044_1034: downloading analytic_sr...\n",
      "20180203_020044_1034: downloading analytic_xml...\n",
      "20180203_031433_0f36: downloading analytic...\n",
      "20180203_031433_0f36: downloading analytic_sr...\n",
      "20180203_031433_0f36: downloading analytic_xml...\n",
      "20180203_031435_0f36: downloading analytic...\n",
      "20180203_031435_0f36: downloading analytic_sr...\n",
      "20180203_031435_0f36: downloading analytic_xml...\n",
      "20180203_031434_0f36: downloading analytic...\n",
      "20180203_031434_0f36: downloading analytic_sr...\n",
      "20180203_031434_0f36: downloading analytic_xml...\n",
      "20180129_033352_0c37: downloading analytic...\n",
      "20180129_033352_0c37: downloading analytic_xml...\n",
      "20180129_033351_1_0c37: downloading analytic...\n",
      "20180129_033351_1_0c37: downloading analytic_xml...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# now check status and download once available\n",
    "# note: as the downloading function is serial in nature, so is this loop\n",
    "if ((status_count[202] + status_count[204]) > 0) and not dry_run:\n",
    "    fIx = 0\n",
    "    for feature in srj[\"features\"]:\n",
    "        assets_url = feature[\"_links\"][\"assets\"]\n",
    "        asset_activated = False\n",
    "        while asset_activated == False:           \n",
    "            res = session.get(assets_url)\n",
    "            # Assign a variable to the item's assets url response\n",
    "            assets = res.json()\n",
    "            for ass in assets_get_list[fIx]:\n",
    "                ass_status = assets[ass][\"status\"]\n",
    "                if ass_status == 'active':\n",
    "                    location_url = assets[ass][\"location\"]\n",
    "                    print(feature[\"id\"] + \": downloading \" + ass + \"...\")\n",
    "                    download_ass(location_url, data_dir=data_dir)\n",
    "                    # don't forget\n",
    "                    asset_activated = True\n",
    "            # wait a bit before next try\n",
    "            time.sleep(1)\n",
    "        # increment index\n",
    "        fIx += 1    \n",
    "    print(\"done\")\n",
    "else:\n",
    "    if not dry_run:\n",
    "        print(\"no single item could be activated or was already activated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when done, close session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
